{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import spacy\r\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use spaCy to create a processed Doc object, which is a container for accessing linguistic annotations, for a given input string:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "introduction_text = ('This tutorial is about Natural'\r\n",
    "...     ' Language Processing in Spacy.')\r\n",
    "introduction_doc = nlp(introduction_text)\r\n",
    "# Extract tokens for the given doc\r\n",
    "print ([token.text for token in introduction_doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example, notice how the text is converted to an object that is understood by spaCy. You can use this method to convert any text into a processed Doc object and deduce attributes, which will be covered in the coming sections."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**READING FROM FILE**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "file_name = 'sreelal.txt'\r\n",
    "introduction_file_text = open(file_name).read()\r\n",
    "introduction_file_doc = nlp(introduction_file_text)\r\n",
    "# Extract tokens for the given doc\r\n",
    "print ([token.text for token in introduction_file_doc])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Hello', ',', 'My', 'name', 'is', 'Sreelal', 'C.', 'I', \"'m\", 'a', 'software', 'Engineer', 'working', 'at', 'Tata', 'Consultancy', 'Services', '.', 'I', \"'m\", 'passionate', 'about', 'technology', 'and', 'loves', 'working', 'in', 'Artificial', 'Intelligence', 'or', 'Machine', 'Learning', '.']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SENTENCE DETECTION**\n",
    "\n",
    "Sentence Detection is the process of locating the start and end of sentences in a given text. This allows you to you divide a text into linguistically meaningful units. You’ll use these units when you’re processing your text to perform tasks such as part of speech tagging and entity extraction.\n",
    "\n",
    "In spaCy, the sents property is used to extract sentences. Here’s how you would extract the total number of sentences and the sentences for a given input text:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "sentences = list(introduction_file_doc.sents)\r\n",
    "print(len(sentences))\r\n",
    "for sentence in sentences:\r\n",
    "    print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "Hello, My name is Sreelal C.\n",
      "I'm a software Engineer working at Tata Consultancy Services.\n",
      "I'm passionate about technology and loves working in Artificial Intelligence or Machine Learning.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the above example, spaCy is correctly able to identify sentences in the English language, using a full stop(.) as the sentence delimiter. You can also customize the sentence detection to detect sentences on custom delimiters.\n",
    "\n",
    "Here’s an example, where an ellipsis(...) is used as the delimiter:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "@Language.component\r\n",
    "def set_custom_boundaries(doc):\r\n",
    "    # Adds support to use `...` as the delimiter for sentence detection\r\n",
    "    for token in doc[:-1]:\r\n",
    "        if token.text == '...':\r\n",
    "             doc[token.i+1].is_sent_start = True\r\n",
    "    return doc\r\n",
    "\r\n",
    "ellipsis_text = ('Gus, can you, ... never mind, I forgot'\r\n",
    "                  ' what I was saying. So, do you think'\r\n",
    "                  ' we should ...')\r\n",
    "# Load a new model instance\r\n",
    "custom_nlp = spacy.load('en_core_web_sm')\r\n",
    "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\r\n",
    "custom_ellipsis_doc = custom_nlp(ellipsis_text)\r\n",
    "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\r\n",
    "for sentence in custom_ellipsis_sentences:\r\n",
    "     print(sentence)\r\n",
    "\r\n",
    "# Sentence Detection with no customization\r\n",
    "ellipsis_doc = nlp(ellipsis_text)\r\n",
    "ellipsis_sentences = list(ellipsis_doc.sents)\r\n",
    "for sentence in ellipsis_sentences:\r\n",
    "     print(sentence)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'Language' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-502b5e1bbf4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;33m@\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mset_custom_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# Adds support to use `...` as the delimiter for sentence detection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'...'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Language' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Tokenization in spaCy**\n",
    "\n",
    "Tokenization is the next step after sentence detection. It allows you to identify the basic units in your text. These basic units are called tokens. Tokenization is useful because it breaks a text into meaningful units. These units are used for further analysis, like part of speech tagging.\n",
    "\n",
    "In spaCy, you can print tokens by iterating on the Doc object:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for token in introduction_file_doc:\r\n",
    "...     print (token, token.idx)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello 0\n",
      ", 5\n",
      "My 7\n",
      "name 10\n",
      "is 15\n",
      "Sreelal 18\n",
      "C. 26\n",
      "I 29\n",
      "'m 30\n",
      "a 33\n",
      "software 35\n",
      "Engineer 44\n",
      "working 53\n",
      "at 61\n",
      "Tata 64\n",
      "Consultancy 69\n",
      "Services 81\n",
      ". 89\n",
      "I 91\n",
      "'m 92\n",
      "passionate 95\n",
      "about 106\n",
      "technology 112\n",
      "and 123\n",
      "loves 127\n",
      "working 133\n",
      "in 141\n",
      "Artificial 144\n",
      "Intelligence 155\n",
      "or 168\n",
      "Machine 171\n",
      "Learning 179\n",
      ". 187\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- **text_with_ws** prints token text with trailing space (if present).\n",
    "- **is_alpha detects** if the token consists of alphabetic characters or not.\n",
    "- **is_punct** detects if the token is a punctuation symbol or not.\n",
    "- **is_space** detects if the token is a space or not.\n",
    "- **shape_ prints** out the shape of the word.\n",
    "- **is_stop** detects if the token is a stop word or not."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "for token in introduction_file_doc:\r\n",
    "    print (token, token.idx, token.text_with_ws, token.is_alpha, token.is_punct, token.is_space, \\\r\n",
    "           token.shape_, token.is_stop)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello 0 Hello True False False Xxxxx False\n",
      ", 5 ,  False True False , False\n",
      "My 7 My  True False False Xx True\n",
      "name 10 name  True False False xxxx True\n",
      "is 15 is  True False False xx True\n",
      "Sreelal 18 Sreelal  True False False Xxxxx False\n",
      "C. 26 C.  False False False X. False\n",
      "I 29 I True False False X True\n",
      "'m 30 'm  False False False 'x True\n",
      "a 33 a  True False False x True\n",
      "software 35 software  True False False xxxx False\n",
      "Engineer 44 Engineer  True False False Xxxxx False\n",
      "working 53 working  True False False xxxx False\n",
      "at 61 at  True False False xx True\n",
      "Tata 64 Tata  True False False Xxxx False\n",
      "Consultancy 69 Consultancy  True False False Xxxxx False\n",
      "Services 81 Services True False False Xxxxx False\n",
      ". 89 .  False True False . False\n",
      "I 91 I True False False X True\n",
      "'m 92 'm  False False False 'x True\n",
      "passionate 95 passionate  True False False xxxx False\n",
      "about 106 about  True False False xxxx True\n",
      "technology 112 technology  True False False xxxx False\n",
      "and 123 and  True False False xxx True\n",
      "loves 127 loves  True False False xxxx False\n",
      "working 133 working  True False False xxxx False\n",
      "in 141 in  True False False xx True\n",
      "Artificial 144 Artificial  True False False Xxxxx False\n",
      "Intelligence 155 Intelligence  True False False Xxxxx False\n",
      "or 168 or  True False False xx True\n",
      "Machine 171 Machine  True False False Xxxxx False\n",
      "Learning 179 Learning True False False Xxxxx False\n",
      ". 187 . False True False . False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**REMOVING STOP WORDS**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "for token in introduction_file_doc:\r\n",
    "    if not token.is_stop:\r\n",
    "        print (token)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello\n",
      ",\n",
      "Sreelal\n",
      "C.\n",
      "software\n",
      "Engineer\n",
      "working\n",
      "Tata\n",
      "Consultancy\n",
      "Services\n",
      ".\n",
      "passionate\n",
      "technology\n",
      "loves\n",
      "working\n",
      "Artificial\n",
      "Intelligence\n",
      "Machine\n",
      "Learning\n",
      ".\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    " about_no_stopword_doc = [token for token in introduction_file_doc if not token.is_stop]\r\n",
    ">>> print (about_no_stopword_doc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Hello, ,, Sreelal, C., software, Engineer, working, Tata, Consultancy, Services, ., passionate, technology, loves, working, Artificial, Intelligence, Machine, Learning, .]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**LEMMATIZATION**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "for token in introduction_file_doc:\r\n",
    "    print(token, token.lemma_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hello hello\n",
      ", ,\n",
      "My my\n",
      "name name\n",
      "is be\n",
      "Sreelal Sreelal\n",
      "C. C.\n",
      "I I\n",
      "'m be\n",
      "a a\n",
      "software software\n",
      "Engineer Engineer\n",
      "working work\n",
      "at at\n",
      "Tata Tata\n",
      "Consultancy Consultancy\n",
      "Services Services\n",
      ". .\n",
      "I I\n",
      "'m be\n",
      "passionate passionate\n",
      "about about\n",
      "technology technology\n",
      "and and\n",
      "loves love\n",
      "working work\n",
      "in in\n",
      "Artificial Artificial\n",
      "Intelligence Intelligence\n",
      "or or\n",
      "Machine Machine\n",
      "Learning Learning\n",
      ". .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from spacy import displacy\r\n",
    "about_interest_text = ('He is interested in learning'\r\n",
    "    ' Natural Language Processing.')\r\n",
    "about_interest_doc = nlp(about_interest_text)\r\n",
    "displacy.render(about_interest_doc, style='ent', jupyter=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">He is interested in learning \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Natural Language Processing\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ".</div></span>"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "interpreter": {
   "hash": "2ebf6bb53b96a26f5fd31c9b3166866123e3c511cac88b436c096329d969a772"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}